{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ab601c",
   "metadata": {},
   "source": [
    "### Robustness experiment for mCMIkNN based on the data generated for ci testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d340b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af752063",
   "metadata": {},
   "outputs": [],
   "source": [
    "### manual added implementation of mCMIkNN\n",
    "from abc import ABC, abstractmethod\n",
    "from math import floor\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.special import digamma\n",
    "from scipy.stats import rankdata\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "class IndependenceTest(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def test_params(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_pval(self, x, y, z):\n",
    "        pass\n",
    "    \n",
    "class mCMIkNN(IndependenceTest):\n",
    "    '''\n",
    "        An independence test class that provides acces to the following non-parametric methods described in Huegle et al. (2022)\n",
    "        - compute_mi: non-parametric estimator for mutual information I(X;Y)\n",
    "        - compute_cmi: non-parametric estimator for conditional mutual information I(X;Y|Z)\n",
    "        - compute_pval_mi: non-parametric independence test returning p value for H_0: X _||_ Y\n",
    "        - compute_pval: non-parametric conditional independence test returing p value for H_0: X _||_ Y | Z\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 kcmi=25,\n",
    "                 kperm=5,\n",
    "                 Mperm=100,\n",
    "                 subsample=None,\n",
    "                 transform=None,\n",
    "                 log_warning=False):\n",
    "\n",
    "        # Required parameters with defaults\n",
    "        self.Mperm = Mperm\n",
    "        self.kcmi = kcmi\n",
    "        self.kperm = kperm\n",
    "        # Persisted values\n",
    "        self.cmi_val = None\n",
    "        self.null_distribution = None\n",
    "        self.permutation = None\n",
    "        self.pval = None\n",
    "        # Options\n",
    "        self.transform = transform\n",
    "        self.dis = 10\n",
    "        self.subsample = subsample\n",
    "        self.leafsize = 16\n",
    "        self.log_warning = log_warning\n",
    "\n",
    "    def test_params(self):\n",
    "        return {\n",
    "            'kcmi': self.kcmi,\n",
    "            'kperm':self.kperm,\n",
    "            'Mperm': self.Mperm,\n",
    "            'Transformation': self.transform,\n",
    "        }\n",
    "\n",
    "    def rank_transform(self, x, y, z=None):\n",
    "        '''\n",
    "            Rank Transform all variables while preserving rank of discrete points.\n",
    "        '''\n",
    "        x_transformed = rankdata(x, method='dense', axis=0).astype(np.float32)\n",
    "        y_transformed = rankdata(y, method='dense', axis=0).astype(np.float32)\n",
    "        z_transformed = None if np.all(z) == None else rankdata(z, method='dense', axis=0).astype(np.float32)\n",
    "        return (x_transformed, y_transformed, z_transformed)\n",
    "\n",
    "    def uniform_transform(self, x, y, z=None):\n",
    "        '''\n",
    "            Transform all variables to take values in [0,1] with equal distances while preserving discrete points (i.e., normalized rank transformed variables).\n",
    "        '''\n",
    "        x_transformed = self.normalize(rankdata(x, method='dense', axis=0).astype(np.float32))\n",
    "        y_transformed = self.normalize(rankdata(y, method='dense', axis=0).astype(np.float32))\n",
    "        z_transformed = None if np.all(z) == None else self.normalize(rankdata(z, method='dense', axis=0).astype(np.float32))\n",
    "        return (x_transformed, y_transformed, z_transformed)\n",
    "\n",
    "    def standardize(self, x):\n",
    "        x_mean = np.mean(x, axis=0)\n",
    "        x_std = np.std(x, axis=0)\n",
    "        return (x - x_mean) / x_std\n",
    "\n",
    "    def standard_transform(self, x, y, z=None):\n",
    "        '''\n",
    "            Standardize all continuous variables (unique values > 10) to std. normal distribution,\n",
    "            i.e., (x-mean(x))/std(x)\n",
    "        '''\n",
    "        res = []\n",
    "        # Standardize Xa if unique(Xa) > 10\n",
    "        xN = []\n",
    "        for a in x.T:\n",
    "            if np.unique(a, axis=0).shape[0] > self.dis:\n",
    "                xN.append(self.standardize(a))\n",
    "            else:\n",
    "                xN.append(a)\n",
    "        res.append(np.asarray(xN).T)\n",
    "\n",
    "        # Standardize Ya if unique(Ya) > 10\n",
    "        yN = []\n",
    "        for a in y.T:\n",
    "            if np.unique(a, axis=0).shape[0] > self.dis:\n",
    "                yN.append(self.standardize(a))\n",
    "            else:\n",
    "                yN.append(a)\n",
    "        res.append(np.asarray(yN).T)\n",
    "\n",
    "        # Standardize Za if unique(Za) > 10\n",
    "        zN = []\n",
    "        if np.all(z) != None:\n",
    "            for a in z.T:\n",
    "                if np.unique(a, axis=0).shape[0] > self.dis:\n",
    "                    zN.append(self.standardize(a))\n",
    "                else:\n",
    "                    zN.append(a)\n",
    "            res.append(np.asarray(zN).T)\n",
    "        else:\n",
    "            res.append(None)\n",
    "        return res\n",
    "\n",
    "    def normalize(self, x):\n",
    "        x_min = np.min(x, axis=0)\n",
    "        x_max = np.max(x, axis=0)\n",
    "        return (x-x_min)/(x_max-x_min)\n",
    "\n",
    "    def normal_transform(self, x, y, z=None):\n",
    "        '''\n",
    "            Normalize all continuous variables (unique values > 10) to take values in [0,1],\n",
    "            i.e., (x-min(x))/(max(x)-min(x))\n",
    "        '''\n",
    "        res = []\n",
    "        # Normalize Xa if unique(Xa) > 10\n",
    "        xN = []\n",
    "        for a in x.T:\n",
    "            if np.unique(a, axis=0).shape[0] > self.dis:\n",
    "                xN.append(self.normalize(a))\n",
    "            else:\n",
    "                xN.append(a)\n",
    "        res.append(np.asarray(xN).T)\n",
    "\n",
    "        # Normalize Ya if unique(Ya) > 10\n",
    "        yN = []\n",
    "        for a in y.T:\n",
    "            if np.unique(a, axis=0).shape[0] > self.dis:\n",
    "                yN.append(self.normalize(a))\n",
    "            else:\n",
    "                yN.append(a)\n",
    "        res.append(np.asarray(yN).T)\n",
    "\n",
    "        # Normalize Za if unique(Za) > 10\n",
    "        zN = []\n",
    "        if np.all(z) != None:\n",
    "            for a in z.T:\n",
    "                if np.unique(a, axis=0).shape[0] > self.dis:\n",
    "                    zN.append(self.normalize(a))\n",
    "                else:\n",
    "                    zN.append(a)\n",
    "            res.append(np.asarray(zN).T)\n",
    "        else:\n",
    "            res.append(None)\n",
    "        return res\n",
    "\n",
    "    def transform_data(self, x, y, z=None):\n",
    "        if self.transform == 'rank':\n",
    "            return self.rank_transform(x, y, z)\n",
    "        elif self.transform == 'standardize':\n",
    "            return self.standard_transform(x, y, z)\n",
    "        elif self.transform == 'normalize':\n",
    "            return self.normal_transform(x, y, z)\n",
    "        elif self.transform == 'uniform':\n",
    "            return self.uniform_transform(x, y, z)\n",
    "        return (x, y, z)\n",
    "\n",
    "    def count_NN(self, tree, points, rho):\n",
    "        '''\n",
    "            Count all nearest neighbors with distance smaller or equal to rho. (Note, this does not include point itself.)\n",
    "        '''\n",
    "        return tree.query_ball_point(points, rho, p=np.inf, return_length=True)-1\n",
    "\n",
    "\n",
    "    def return_NN(self, tree, points, sigma):\n",
    "        '''\n",
    "            Return all nearest neighbors with distance smaller or equal to sigma. (Note, excludes point itself.)\n",
    "        '''\n",
    "        # includes points itself\n",
    "        neighbors = tree.query_ball_point(points, sigma, p=np.inf)\n",
    "        # exclude points withing neighborhood\n",
    "        for i in range(len(points)):\n",
    "            neighbors[i].remove(i)\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "\n",
    "    def compute_mi(self, x, y):\n",
    "        '''\n",
    "            Estimate the mutual information I(X;Y) of X and Y from n samples (x_i, y_i)_{i=1}^n\n",
    "            using Alg. 1 which relates to the *Mixed-KSG* mutual information estimator of Gao et al. (2017)\n",
    "\n",
    "            Note: Using digamma instead of log according to Mesner et al. (2021)\n",
    "\n",
    "            Input:  x: 2D array of size n*dz (or 1D list of size n if dx = 1)\n",
    "                    y: 2D array of size n*dz (or 1D list of size n if dy = 1)\n",
    "                    (self.kcmi: k-nearest neighbor parameter)\n",
    "\n",
    "            Output: ^I_n(X;Y)\n",
    "        '''\n",
    "        assert len(x) == len(y), \"x and y should have the same number of observations\"\n",
    "        n = len(x)\n",
    "        assert self.kcmi <= n-1, \"Set kcmi smaller than number of observations - 1\"\n",
    "\n",
    "        x =  x.reshape((n, 1)).astype(np.float32) if (x.shape == (n,)) else x.astype(np.float32)\n",
    "        y =  y.reshape((n, 1)).astype(np.float32) if (y.shape == (n,)) else y.astype(np.float32)\n",
    "        xy = np.concatenate((x, y), axis=1)\n",
    "\n",
    "        # build k-d trees\n",
    "        tree_xy = KDTree(xy, leafsize=self.leafsize)\n",
    "        tree_x = KDTree(x, leafsize=self.leafsize)\n",
    "        tree_y = KDTree(y, leafsize=self.leafsize)\n",
    "\n",
    "        # compute k-NN distances, using k+1 as this includes dist to self\n",
    "        rho = tree_xy.query(xy, self.kcmi+1, p=np.inf)[0][:, self.kcmi]\n",
    "\n",
    "        # if continous  -> k_tilde = k_cmi\n",
    "        # if discrete or mixed -> k_tilde = number of samples with distance rho\n",
    "        k_tilde = self.count_NN(tree_xy, xy, rho)\n",
    "\n",
    "        # entropy estimates - i.e., count points withon distance rho\n",
    "        nx = self.count_NN(tree_x, x, rho)\n",
    "        ny = self.count_NN(tree_y, y, rho)\n",
    "\n",
    "        mi = np.mean(digamma(k_tilde) + digamma(n) - digamma(nx) - digamma(ny))\n",
    "        return max(0,mi)\n",
    "\n",
    "    def compute_cmi(self, x, y, z):\n",
    "        '''\n",
    "            Estimate the conditional mutual information I(X;Y|Z) of X and Y given a dz-dimensional variable Z from samples (x_i, y_i,z_i)_{i=1}^n\n",
    "            Using Alg. 1 which relates to the *Mixed-KSG* mutual information estimator of Mesner et al. (2021)\n",
    "\n",
    "            Input:  x: 2D array of size n*dz (or 1D list of size n if dx = 1)\n",
    "                    y: 2D array of size n*dz (or 1D list of size n if dy = 1)\n",
    "                    z: 2D array of size n*dz (or 1D list of size n if dz = 1)\n",
    "                    (self.kcmi: k-nearest neighbor parameter)\n",
    "\n",
    "            Output: ^I_n(X;Y|Z)\n",
    "        '''\n",
    "        assert len(x) == len(y) == len(z), \"x, y, and z should have same number of observations\"\n",
    "        n=len(x)\n",
    "        assert self.kcmi <= n-1, \"Set kcmi smaller than number of observations - 1\"\n",
    "\n",
    "        x =  x.reshape((n, 1)).astype(np.float32) if (x.shape == (n,)) else x.astype(np.float32)\n",
    "        y =  y.reshape((n, 1)).astype(np.float32) if (y.shape == (n,)) else y.astype(np.float32)\n",
    "        z =  z.reshape((n, 1)).astype(np.float32) if (z.shape == (n,)) else z.astype(np.float32)\n",
    "\n",
    "        yz = np.concatenate((y, z), axis=1)\n",
    "        xyz = np.concatenate((x, yz), axis=1)\n",
    "        xz = np.concatenate((x, z), axis=1)\n",
    "\n",
    "        # build k-d trees\n",
    "        tree_xyz = KDTree(xyz, leafsize= self.leafsize)\n",
    "        tree_xz = KDTree(xz, leafsize= self.leafsize)\n",
    "        tree_yz = KDTree(yz, leafsize= self.leafsize)\n",
    "        tree_z = KDTree(z, leafsize= self.leafsize)\n",
    "\n",
    "        # compute k-NN distances, using k+1 as this includes dist to self\n",
    "        rho = tree_xyz.query(xyz, self.kcmi+1, p=np.inf)[0][:, self.kcmi]\n",
    "\n",
    "        # if continous  -> k_tilde = k_cmi\n",
    "        # if discrete or mixed -> k_tilde = number of samples with distance rho\n",
    "        k_tilde = self.count_NN(tree_xyz, xyz, rho)\n",
    "\n",
    "        # entropy estimates - i.e., count neighbors within distance rho\n",
    "        nxz = self.count_NN(tree_xz, xz, rho)\n",
    "        nyz = self.count_NN(tree_yz, yz, rho)\n",
    "        nz = self.count_NN(tree_z, z, rho)\n",
    "\n",
    "        cmi = np.mean(digamma(k_tilde) - digamma(nxz) - digamma(nyz) + digamma(nz))\n",
    "        return max(0,cmi)\n",
    "\n",
    "    def compute_pval_mi(self, x, y):\n",
    "        '''\n",
    "            Returns the p value returning p value for H_0: X _||_ Y estimated from n samples (x_i, y_i)_{i=1}^n\n",
    "            using Alg. 2 of Huegle et al. (2022), i.e., comparing the present MI against MIs for shuffled samples of X under H_0.\n",
    "\n",
    "            H_0: X and Y are independent\n",
    "            H_1: X and Y are dependent\n",
    "\n",
    "            Note: Using rank transformation for k-NN searches according to Runge (2017) but preserving ties\n",
    "\n",
    "            Input:  x: 2D array of size n*dz (or 1D list of size n if dx = 1)\n",
    "                    y: 2D array of size n*dz (or 1D list of size n if dy = 1)\n",
    "                    (self.Mperm: number of permutations)\n",
    "                    (self.kcmi: k used for MI estimation)\n",
    "\n",
    "            Output: p_perm,n\n",
    "        '''\n",
    "\n",
    "        assert len(x) == len(y), \"x and y should have same number of observations\"\n",
    "\n",
    "        if self.subsample is not None:\n",
    "            sample = np.random.choice(np.arange(len(x)), min(len(x), self.subsample), replace=False)\n",
    "            x, y = x[sample], y[sample]\n",
    "\n",
    "        n = len(x)\n",
    "\n",
    "        x =  x.reshape((n, 1)).astype(np.float32) if (x.shape == (n,)) else x.astype(np.float32)\n",
    "        y =  y.reshape((n, 1)).astype(np.float32) if (y.shape == (n,)) else y.astype(np.float32)\n",
    "\n",
    "        x, y, _ = self.transform_data(x,y)\n",
    "\n",
    "        n = len(x)\n",
    "        if self.kperm == 0:\n",
    "            self.kperm = np.floor(np.sqrt(n)).astype(int)\n",
    "        elif 0 < self.kperm <= 1:\n",
    "            self.kperm = np.floor(np.nextafter(self.kperm,0) * n).astype(int) # ensure kperm < n (n-1 equals shuffling without considering z)\n",
    "\n",
    "        if self.kcmi == 0:\n",
    "            self.kcmi = np.floor(np.sqrt(n)).astype(int)\n",
    "        elif 0 < self.kcmi <= 1:\n",
    "            self.kcmi = np.floor(np.nextafter(self.kcmi,0) * n).astype(int) # ensure kcmi < n\n",
    "\n",
    "        # estimate present MI value\n",
    "        self.cmi_val = self.compute_mi(x, y)\n",
    "\n",
    "        # estimate Mperm MIs for shuffled X under H_0\n",
    "        null_dist = np.zeros(self.Mperm)\n",
    "        for m in range(self.Mperm):\n",
    "             # Generate random shuffled x\n",
    "            x_shuffled = x[np.random.default_rng().permutation(n)]\n",
    "            null_dist[m] = self.compute_mi(x_shuffled, y)\n",
    "\n",
    "        # estimate pvalue comparing MI against MIs for shuffled X\n",
    "        self.null_distribution = null_dist\n",
    "        self.pval = (1+np.sum(null_dist >= self.cmi_val))/(1+self.Mperm)\n",
    "        return self.pval\n",
    "\n",
    "    def compute_pval(self, x, y, z=None):\n",
    "        '''\n",
    "            Returns the p value returning p value for H_0: X _||_ Y | Z estimated from n samples (x_i, y_i,z_i)_{i=1}^n\n",
    "            using Alg. 2 of Huegle et al. (2022), i.e., comparing the present MI against MIs for shuffled samples of X under H_0.\n",
    "\n",
    "            H_0: X and Y are independent given dz dimensional Z\n",
    "            H_1: X and Y are dependent given dz dimensional Z\n",
    "\n",
    "            Note: Using rank transformation for k-NN searches according to Runge (2017) but preserving ties\n",
    "\n",
    "            Input:  x: 1D 2D array of size n*dz (or 1D list of size n if dx = 1)\n",
    "                    y: 1D 2D array of size n*dz (or 1D list of size n if dy = 1)\n",
    "                    z: 2D array of size n*dz (or 1D list of size n if dz = 1)\n",
    "                    (self.Mperm: number of permutations)\n",
    "                    (self.kperm: k used for local permuation scheme)\n",
    "                    (self.kcmi: k used for MI estimation)\n",
    "\n",
    "            Output: p_perm,n\n",
    "        '''\n",
    "\n",
    "        # for empty z calculate return p value according to H_0: X _||_ Y\n",
    "        if z is None:\n",
    "            return self.compute_pval_mi(x, y)\n",
    "\n",
    "        assert len(x) == len(y) == len(z), \"x, y, and z should have same number of observations\"\n",
    "\n",
    "        if self.subsample is not None:\n",
    "            sample = np.random.choice(np.arange(len(x)), min(len(x), self.subsample), replace=False)\n",
    "            x, y, z = x[sample], y[sample], z[sample]\n",
    "\n",
    "        n = len(x)\n",
    "\n",
    "        x =  x.reshape((n, 1)).astype(np.float32) if (x.shape == (n,)) else x.astype(np.float32)\n",
    "        y =  y.reshape((n, 1)).astype(np.float32) if (y.shape == (n,)) else y.astype(np.float32)\n",
    "        z =  z.reshape((n, 1)).astype(np.float32) if (z.shape == (n,)) else z.astype(np.float32)\n",
    "        x, y, z = self.transform_data(x, y, z)\n",
    "\n",
    "        if self.kperm == 0:\n",
    "            self.kperm = np.floor(np.sqrt(n)).astype(int)\n",
    "        elif 0 < self.kperm <= 1:\n",
    "            self.kperm = np.floor(np.nextafter(self.kperm,0) * n).astype(int) # ensure kperm < n (n-1 equals shuffling without considering z)\n",
    "\n",
    "        if self.kcmi == 0:\n",
    "            self.kcmi = np.floor(np.sqrt(n)).astype(int)\n",
    "        elif 0 < self.kcmi <= 1:\n",
    "            self.kcmi = np.floor(np.nextafter(self.kcmi,0) * n).astype(int) # ensure kcmi < n\n",
    "\n",
    "        # estimate present CMI value\n",
    "        self.cmi_val = self.compute_cmi(x, y, z)\n",
    "\n",
    "        # Get nearest neighbors around each sample point in Z\n",
    "        tree_z = KDTree(z, leafsize= self.leafsize)\n",
    "\n",
    "        # compute k-NN distances in Z, using k+1 as this includes dist to self\n",
    "        sigma = tree_z.query(z, self.kperm+1, p=np.inf)[0][:, self.kperm]\n",
    "\n",
    "        # if continuous -> k points distance smaller or equal to sigma excluding the point itself\n",
    "        # if discrete or mixed -> all points with distance smaller or euqla to the k-NN distance sigma excluding the point itself\n",
    "        neighbors = self.return_NN(tree_z, z, sigma)\n",
    "\n",
    "        # estimate Mperm CMIs for shuffled X under H_0 while preserving marginal distributions\n",
    "        null_dist = np.zeros(self.Mperm)\n",
    "        for m in range(self.Mperm):\n",
    "            # compose local permutations of nearest neighbors to receive a restricted permutation of the whole index list\n",
    "            permutation = np.arange(n)\n",
    "            for i in range(n-1,-1,-1):\n",
    "                permutation[neighbors[i,]]=permutation[np.random.default_rng().permutation(neighbors[i,])]\n",
    "            x_shuffled = x[permutation]\n",
    "            null_dist[m] = self.compute_cmi(x_shuffled, y, z)\n",
    "\n",
    "        self.null_distribution = null_dist\n",
    "        self.pval = (1+np.sum(null_dist >= self.cmi_val))/(1+self.Mperm)\n",
    "        return self.pval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0fab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameters\n",
    "### number of cores for parallel execution\n",
    "processes = max(1, multiprocessing.cpu_count())\n",
    "\n",
    "kcmi = 25\n",
    "kperm = 5\n",
    "Mperm = 100\n",
    "\n",
    "try:\n",
    "    os.mkdir('./parts_mCMIkNN/')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41dc7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ciTesting(dataRun):\n",
    "    pid = multiprocessing.current_process().pid\n",
    "    with open('./parts_mCMIkNN/mCMIkNN'+str(pid)+'.csv','a') as file:\n",
    "        ### import icsl and setup test\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        filename = os.fsdecode(dataRun)\n",
    "        splitted = filename.split('_')\n",
    "        cgmid = int(splitted[0])\n",
    "        dnr = float(splitted[2]+'.'+splitted[3])\n",
    "        s = int(splitted[5]) #sepsetsize\n",
    "        hasedge = (splitted[6] == 'withEdge')\n",
    "        df = pd.read_csv('../data_generation/ci_data_normalized/'+filename)\n",
    "        sample = splitted[7].split('.')[0]\n",
    "        print(filename, sample, str(pid))\n",
    "        npdfs = df.to_numpy()\n",
    "        citest = mCMIkNN(kcmi=kcmi,kperm=kperm, Mperm=Mperm)\n",
    "        pvalue = citest.compute_pval(npdfs[:,[s]], npdfs[:,[s+1]], z=npdfs[:,[u for u in range(s)]])\n",
    "        writer.writerow([cgmid, sample, s, dnr, 'mCMIkNN', pvalue, hasedge])\n",
    "        file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### parallel computation\n",
    "directory = os.fsencode('../data_generation/ci_data_normalized/')\n",
    "with Pool(processes=processes) as pool:\n",
    "    result = pool.map(ciTesting, os.listdir(directory))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d298e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### combining partial results from parallel execution\n",
    "dfs = []\n",
    "for file in os.listdir(\"./parts_mCMIkNN/\"):\n",
    "    if 'mCMIkNN' in file:\n",
    "        dfs.append(pd.read_csv('./parts_mCMIkNN/'+file, names=['cgmid','samples','sepsetsize','discretenoderatio','method','pvalue','hasedge']))\n",
    "\n",
    "df = pd.concat(dfs,ignore_index=True)\n",
    "df.to_csv(\"./robustness.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0b007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
